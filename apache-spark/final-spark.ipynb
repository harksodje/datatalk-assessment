{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathlib\n",
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "# from typing import List\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('../apache-spark/data')\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName('adis').getOrCreate()\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "# disable summary metadata\n",
    "spark.conf.set('spark.sql.parquet.mergeSchema', 'false')\n",
    "\n",
    "# spark.hadoopConfiguration.set(\"parquet.enable.summary-metadata\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_infer_schema(file: str, file_extension: str):\n",
    "    \n",
    "    if file_extension == \"csv\":\n",
    "        df = pd.read_csv(file, nrows=5)\n",
    "        # print(df.head(5))\n",
    "        # schema = df.types\n",
    "    df_s = spark.createDataFrame(df)\n",
    "    schema = df_s.schema\n",
    "    return schema\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropOff_datetime', StringType(), True), StructField('PUlocationID', LongType(), True), StructField('DOlocationID', LongType(), True), StructField('SR_Flag', DoubleType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_df = pd_infer_schema(data_path / \"fhv_tripdata_2019-10.csv.gz\", 'csv')\n",
    "schema_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"dispatching_base_num\", T.StringType(), True),\n",
    "        T.StructField(\"pickup_date\", T.TimestampType(), True),\n",
    "        T.StructField(\"dropOff_datetime\", T.TimestampType(), True),\n",
    "        T.StructField(\"PUlocationID\", T.IntegerType(), True),\n",
    "        T.StructField(\"DOlocationID\", T.IntegerType(), True),\n",
    "        T.StructField(\"SR_Flag\", T.StringType(), True),\n",
    "        # T.StructField(\"Dispatching_base_number\", T.StringType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_date: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark.read.csv(str(data_path / \"*\"), header=True, schema=data_schema)\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_number: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- sr_flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark  = df_spark \\\n",
    "    .withColumnRenamed(\"dispatching_base_num\", \"dispatching_base_number\") \\\n",
    "    .withColumnRenamed(\"pickup_date\", \"pickup_datetime\") \\\n",
    "    .withColumnRenamed(\"dropOff_datetime\", \"dropoff_datetime\") \\\n",
    "    .withColumnRenamed(\"PUlocationID\", \"pickup_location_id\") \\\n",
    "    .withColumnRenamed(\"DOlocationID\", \"dropoff_location_id\") \\\n",
    "    .withColumnRenamed(\"SR_Flag\", \"sr_flag\") \\\n",
    "    .withColumnRenamed(\"Dispatching_base_number\", \"dispatching_base_number\")\n",
    "    \n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spark.select(\n",
    "#     [\n",
    "#         F.count(F.col(col).isNull()).alias(f\"{col}_null\") for col in df_spark.columns\n",
    "#     ]\n",
    "# ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temp view\n",
    "df_sql_table = df_spark.createOrReplaceTempView(\"fhv_tripdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_number: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- sr_flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 36M\n",
      "-rwxrwxrwx 1 root root    0 Feb 27 11:47 _SUCCESS\n",
      "-rwxrwxrwx 1 root root 6.0M Feb 27 11:47 part-00000-1120756e-6103-4db1-a3b5-e09111852157-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 root root 6.0M Feb 27 11:47 part-00001-1120756e-6103-4db1-a3b5-e09111852157-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 root root 6.0M Feb 27 11:47 part-00002-1120756e-6103-4db1-a3b5-e09111852157-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 root root 6.0M Feb 27 11:47 part-00003-1120756e-6103-4db1-a3b5-e09111852157-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 root root 6.0M Feb 27 11:47 part-00004-1120756e-6103-4db1-a3b5-e09111852157-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 root root 6.0M Feb 27 11:47 part-00005-1120756e-6103-4db1-a3b5-e09111852157-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ../apache-spark/data/fhv_tripdata_2019-10/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|trip_oct_i5|\n",
      "+-----------+\n",
      "|      62610|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# obatain the count of trip on 2019-10-15\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select count(*) as trip_oct_i5 from fhv_tripdata\n",
    "    where date(pickup_datetime) = '2019-10-15'\n",
    "    \n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+------------------+-------------------+-------+\n",
      "|dispatching_base_number|    pickup_datetime|   dropoff_datetime|pickup_location_id|dropoff_location_id|sr_flag|\n",
      "+-----------------------+-------------------+-------------------+------------------+-------------------+-------+\n",
      "|                 B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|               264|                264|   NULL|\n",
      "|                 B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|               264|                264|   NULL|\n",
      "|                 B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|               264|                264|   NULL|\n",
      "|                 B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|               264|                264|   NULL|\n",
      "|                 B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|               264|                264|   NULL|\n",
      "+-----------------------+-------------------+-------------------+------------------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select * from fhv_tripdata\n",
    "    limit 5\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|trip_hours|\n",
      "+----------+\n",
      "|    631152|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# evaluate the longest trip in hourss\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    with trip_hours as (\n",
    "        select DATEDIFF(hour, pickup_datetime, dropoff_datetime) as trip_hours\n",
    "        from fhv_tripdata\n",
    "    )\n",
    "    \n",
    "    select \n",
    "        trip_hours \n",
    "    from trip_hours \n",
    "        order by trip_hours desc limit 1\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup = spark.read.csv(str(data_path / \"taxi_zone_lookup.csv\"), header=True)\n",
    "df_lookup = df_lookup \\\n",
    "    .withColumnRenamed(\"LocationID\", \"location_id\") \\\n",
    "    .withColumnRenamed(\"Borough\", \"borough\") \\\n",
    "    \n",
    "df_lookup.createOrReplaceTempView(\"taxi_zone_lookup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- location_id: string (nullable = true)\n",
      " |-- borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- dispatching_base_number: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- sr_flag: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lookup.printSchema(), df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|       zone|pickup_zone_count|\n",
      "+-----------+-----------------+\n",
      "|Jamaica Bay|                1|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# obtain the zone with the least pickup\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "        select \n",
    "            pickup.zone, count(pickup.zone) as pickup_zone_count \n",
    "        from \n",
    "            fhv_tripdata as fhv\n",
    "        join \n",
    "            taxi_zone_lookup as pickup\n",
    "        where \n",
    "            fhv.pickup_location_id = pickup.location_id\n",
    "        group by \n",
    "            1\n",
    "        order by\n",
    "            2 asc\n",
    "        limit 1\n",
    "    \"\"\"\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
